{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **_keras_**\n",
    "\n",
    "> Keras is an open source neural network library written in Python. It is capable of running on top of MXNet, Deeplearning4j, Tensorflow, CNTK or Theano."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **_scikit-Learn_**\n",
    "The scikit-learn library in Python is built upon the SciPy stack for efficient numerical computation. It is a fully featured library for general machine learning and provides many utilities that are useful in the development of deep learning models. Not least:\n",
    "- Evaluation of models using resampling methods like k-fold cross validation.\n",
    "- Efficient search and evaluation of model hyper-parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Liberies And Modules \n",
    " - we'll import the CNN layers from Keras. These are the convolutional layers that will help us efficiently train on image data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **_Optimizers_**\n",
    "- An optimizer is one of the two arguments required for compiling a Keras model:\n",
    "- SGD Stochastic gradient descent optimizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utils \n",
    "we'll import some utilities. This will help us transform our data later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import np_utils\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "# Split Train/Test\n",
    "x_train, x_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.33)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting started with the Keras Sequential model\n",
    "> https://keras.io/getting-started/sequential-model-guide/\n",
    "> The simplest type of model is the Sequential model, a linear stack of layers. For more complex architectures, you should use the Keras functional API, which allows to build arbitrary graphs of layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build a simple model\n",
    "model = Sequential()\n",
    "model.add(Dense(64, activation='relu', input_dim=x_train.shape[1]))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(len(set(iris.target)), activation='softmax'))\n",
    "\n",
    "# Getting started with the Keras Sequential model\n",
    "model.compile(optimizer='rmsprop',\n",
    "             loss='categorical_crossentropy',\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### epoch \n",
    "- epochs means how many times you go through your training set.\n",
    "- Batch: a set of N samples. The samples in a batch are processed independently, in parallel. If training, a batch results in only one update to the model.\n",
    "\n",
    "see in detail on the this link https://keras.io/getting-started/faq/#what-does-sample-batch-epoch-mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set parameters\n",
    "epoch = 100\n",
    "batch_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "100/100 [==============================] - 0s 902us/step - loss: 0.1038 - acc: 0.9500\n",
      "Epoch 2/100\n",
      "100/100 [==============================] - 0s 722us/step - loss: 0.1631 - acc: 0.9600\n",
      "Epoch 3/100\n",
      "100/100 [==============================] - 0s 993us/step - loss: 0.0822 - acc: 0.9800\n",
      "Epoch 4/100\n",
      "100/100 [==============================] - 0s 812us/step - loss: 0.1047 - acc: 0.9600\n",
      "Epoch 5/100\n",
      "100/100 [==============================] - 0s 968us/step - loss: 0.0972 - acc: 0.9700\n",
      "Epoch 6/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1312 - acc: 0.9600\n",
      "Epoch 7/100\n",
      "100/100 [==============================] - 0s 867us/step - loss: 0.1273 - acc: 0.9700\n",
      "Epoch 8/100\n",
      "100/100 [==============================] - 0s 887us/step - loss: 0.1563 - acc: 0.9500\n",
      "Epoch 9/100\n",
      "100/100 [==============================] - 0s 998us/step - loss: 0.0966 - acc: 0.9500\n",
      "Epoch 10/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1296 - acc: 0.9500\n",
      "Epoch 11/100\n",
      "100/100 [==============================] - 0s 978us/step - loss: 0.0810 - acc: 0.9600\n",
      "Epoch 12/100\n",
      "100/100 [==============================] - 0s 897us/step - loss: 0.0919 - acc: 0.9600\n",
      "Epoch 13/100\n",
      "100/100 [==============================] - 0s 953us/step - loss: 0.1066 - acc: 0.9700\n",
      "Epoch 14/100\n",
      "100/100 [==============================] - 0s 902us/step - loss: 0.0826 - acc: 0.9800\n",
      "Epoch 15/100\n",
      "100/100 [==============================] - 0s 948us/step - loss: 0.1469 - acc: 0.9500\n",
      "Epoch 16/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.0984 - acc: 0.9700\n",
      "Epoch 17/100\n",
      "100/100 [==============================] - 0s 607us/step - loss: 0.0914 - acc: 0.9800\n",
      "Epoch 18/100\n",
      "100/100 [==============================] - 0s 682us/step - loss: 0.0978 - acc: 0.9700\n",
      "Epoch 19/100\n",
      "100/100 [==============================] - 0s 672us/step - loss: 0.1201 - acc: 0.9500\n",
      "Epoch 20/100\n",
      "100/100 [==============================] - 0s 732us/step - loss: 0.1401 - acc: 0.9400\n",
      "Epoch 21/100\n",
      "100/100 [==============================] - 0s 787us/step - loss: 0.1250 - acc: 0.9700\n",
      "Epoch 22/100\n",
      "100/100 [==============================] - 0s 702us/step - loss: 0.1171 - acc: 0.9600\n",
      "Epoch 23/100\n",
      "100/100 [==============================] - 0s 827us/step - loss: 0.0873 - acc: 0.9600\n",
      "Epoch 24/100\n",
      "100/100 [==============================] - 0s 582us/step - loss: 0.1104 - acc: 0.9600\n",
      "Epoch 25/100\n",
      "100/100 [==============================] - 0s 516us/step - loss: 0.0687 - acc: 0.9600\n",
      "Epoch 26/100\n",
      "100/100 [==============================] - 0s 536us/step - loss: 0.1187 - acc: 0.9800\n",
      "Epoch 27/100\n",
      "100/100 [==============================] - 0s 521us/step - loss: 0.0901 - acc: 0.9800\n",
      "Epoch 28/100\n",
      "100/100 [==============================] - 0s 692us/step - loss: 0.1450 - acc: 0.9600\n",
      "Epoch 29/100\n",
      "100/100 [==============================] - 0s 582us/step - loss: 0.0817 - acc: 0.9900\n",
      "Epoch 30/100\n",
      "100/100 [==============================] - 0s 531us/step - loss: 0.1066 - acc: 0.9800\n",
      "Epoch 31/100\n",
      "100/100 [==============================] - 0s 441us/step - loss: 0.0761 - acc: 0.9700\n",
      "Epoch 32/100\n",
      "100/100 [==============================] - 0s 531us/step - loss: 0.1157 - acc: 0.9700\n",
      "Epoch 33/100\n",
      "100/100 [==============================] - 0s 556us/step - loss: 0.0914 - acc: 0.9700\n",
      "Epoch 34/100\n",
      "100/100 [==============================] - 0s 511us/step - loss: 0.1199 - acc: 0.9600\n",
      "Epoch 35/100\n",
      "100/100 [==============================] - 0s 506us/step - loss: 0.1095 - acc: 0.9700\n",
      "Epoch 36/100\n",
      "100/100 [==============================] - 0s 476us/step - loss: 0.0716 - acc: 0.9800\n",
      "Epoch 37/100\n",
      "100/100 [==============================] - 0s 732us/step - loss: 0.0982 - acc: 0.9600\n",
      "Epoch 38/100\n",
      "100/100 [==============================] - 0s 441us/step - loss: 0.1320 - acc: 0.9500\n",
      "Epoch 39/100\n",
      "100/100 [==============================] - 0s 436us/step - loss: 0.0698 - acc: 0.9700\n",
      "Epoch 40/100\n",
      "100/100 [==============================] - 0s 386us/step - loss: 0.1330 - acc: 0.9500\n",
      "Epoch 41/100\n",
      "100/100 [==============================] - 0s 607us/step - loss: 0.1084 - acc: 0.9500\n",
      "Epoch 42/100\n",
      "100/100 [==============================] - 0s 446us/step - loss: 0.1223 - acc: 0.9700\n",
      "Epoch 43/100\n",
      "100/100 [==============================] - 0s 366us/step - loss: 0.1286 - acc: 0.9600\n",
      "Epoch 44/100\n",
      "100/100 [==============================] - 0s 431us/step - loss: 0.0970 - acc: 0.9800\n",
      "Epoch 45/100\n",
      "100/100 [==============================] - 0s 411us/step - loss: 0.1184 - acc: 0.9700\n",
      "Epoch 46/100\n",
      "100/100 [==============================] - 0s 376us/step - loss: 0.1038 - acc: 0.9700\n",
      "Epoch 47/100\n",
      "100/100 [==============================] - 0s 426us/step - loss: 0.1148 - acc: 0.9500\n",
      "Epoch 48/100\n",
      "100/100 [==============================] - 0s 456us/step - loss: 0.0422 - acc: 0.9700\n",
      "Epoch 49/100\n",
      "100/100 [==============================] - 0s 386us/step - loss: 0.2163 - acc: 0.9500\n",
      "Epoch 50/100\n",
      "100/100 [==============================] - 0s 401us/step - loss: 0.1151 - acc: 0.9600\n",
      "Epoch 51/100\n",
      "100/100 [==============================] - 0s 426us/step - loss: 0.1072 - acc: 0.9600\n",
      "Epoch 52/100\n",
      "100/100 [==============================] - 0s 421us/step - loss: 0.0963 - acc: 0.9700\n",
      "Epoch 53/100\n",
      "100/100 [==============================] - 0s 456us/step - loss: 0.0807 - acc: 0.9800\n",
      "Epoch 54/100\n",
      "100/100 [==============================] - 0s 346us/step - loss: 0.0887 - acc: 0.9800\n",
      "Epoch 55/100\n",
      "100/100 [==============================] - 0s 311us/step - loss: 0.1457 - acc: 0.9500\n",
      "Epoch 56/100\n",
      "100/100 [==============================] - 0s 366us/step - loss: 0.0960 - acc: 0.9800\n",
      "Epoch 57/100\n",
      "100/100 [==============================] - 0s 361us/step - loss: 0.0823 - acc: 0.9800\n",
      "Epoch 58/100\n",
      "100/100 [==============================] - 0s 381us/step - loss: 0.0852 - acc: 0.9700\n",
      "Epoch 59/100\n",
      "100/100 [==============================] - 0s 411us/step - loss: 0.1235 - acc: 0.9700\n",
      "Epoch 60/100\n",
      "100/100 [==============================] - 0s 361us/step - loss: 0.1244 - acc: 0.9400\n",
      "Epoch 61/100\n",
      "100/100 [==============================] - 0s 321us/step - loss: 0.1022 - acc: 0.9700\n",
      "Epoch 62/100\n",
      "100/100 [==============================] - 0s 376us/step - loss: 0.0927 - acc: 0.9600\n",
      "Epoch 63/100\n",
      "100/100 [==============================] - 0s 356us/step - loss: 0.1212 - acc: 0.9600\n",
      "Epoch 64/100\n",
      "100/100 [==============================] - 0s 326us/step - loss: 0.0683 - acc: 0.9700\n",
      "Epoch 65/100\n",
      "100/100 [==============================] - 0s 391us/step - loss: 0.0618 - acc: 0.9800\n",
      "Epoch 66/100\n",
      "100/100 [==============================] - 0s 351us/step - loss: 0.1170 - acc: 0.9700\n",
      "Epoch 67/100\n",
      "100/100 [==============================] - 0s 346us/step - loss: 0.1052 - acc: 0.9700\n",
      "Epoch 68/100\n",
      "100/100 [==============================] - 0s 361us/step - loss: 0.0752 - acc: 0.9800\n",
      "Epoch 69/100\n",
      "100/100 [==============================] - 0s 401us/step - loss: 0.0727 - acc: 0.9800\n",
      "Epoch 70/100\n",
      "100/100 [==============================] - 0s 341us/step - loss: 0.1556 - acc: 0.9600\n",
      "Epoch 71/100\n",
      "100/100 [==============================] - 0s 351us/step - loss: 0.1427 - acc: 0.9700\n",
      "Epoch 72/100\n",
      "100/100 [==============================] - 0s 371us/step - loss: 0.0646 - acc: 0.9800\n",
      "Epoch 73/100\n",
      "100/100 [==============================] - 0s 396us/step - loss: 0.1044 - acc: 0.9700\n",
      "Epoch 74/100\n",
      "100/100 [==============================] - 0s 331us/step - loss: 0.0932 - acc: 0.9800\n",
      "Epoch 75/100\n",
      "100/100 [==============================] - 0s 306us/step - loss: 0.0955 - acc: 0.9500\n",
      "Epoch 76/100\n",
      "100/100 [==============================] - 0s 296us/step - loss: 0.0548 - acc: 0.9700\n",
      "Epoch 77/100\n",
      "100/100 [==============================] - 0s 346us/step - loss: 0.1175 - acc: 0.9700\n",
      "Epoch 78/100\n",
      "100/100 [==============================] - 0s 316us/step - loss: 0.1699 - acc: 0.9700\n",
      "Epoch 79/100\n",
      "100/100 [==============================] - 0s 316us/step - loss: 0.0707 - acc: 0.9800\n",
      "Epoch 80/100\n",
      "100/100 [==============================] - 0s 281us/step - loss: 0.1014 - acc: 0.9600\n",
      "Epoch 81/100\n",
      "100/100 [==============================] - 0s 341us/step - loss: 0.0953 - acc: 0.9700\n",
      "Epoch 82/100\n",
      "100/100 [==============================] - 0s 271us/step - loss: 0.1128 - acc: 0.9700\n",
      "Epoch 83/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 0s 331us/step - loss: 0.0492 - acc: 0.9900\n",
      "Epoch 84/100\n",
      "100/100 [==============================] - 0s 341us/step - loss: 0.0486 - acc: 0.9800\n",
      "Epoch 85/100\n",
      "100/100 [==============================] - 0s 291us/step - loss: 0.1050 - acc: 0.9700\n",
      "Epoch 86/100\n",
      "100/100 [==============================] - 0s 411us/step - loss: 0.0953 - acc: 0.9600\n",
      "Epoch 87/100\n",
      "100/100 [==============================] - 0s 351us/step - loss: 0.1489 - acc: 0.9600\n",
      "Epoch 88/100\n",
      "100/100 [==============================] - 0s 351us/step - loss: 0.1065 - acc: 0.9700\n",
      "Epoch 89/100\n",
      "100/100 [==============================] - 0s 376us/step - loss: 0.0908 - acc: 0.9600\n",
      "Epoch 90/100\n",
      "100/100 [==============================] - 0s 341us/step - loss: 0.0999 - acc: 0.9700\n",
      "Epoch 91/100\n",
      "100/100 [==============================] - 0s 361us/step - loss: 0.1216 - acc: 0.9500\n",
      "Epoch 92/100\n",
      "100/100 [==============================] - 0s 321us/step - loss: 0.1324 - acc: 0.9300\n",
      "Epoch 93/100\n",
      "100/100 [==============================] - 0s 376us/step - loss: 0.0860 - acc: 0.9700\n",
      "Epoch 94/100\n",
      "100/100 [==============================] - 0s 351us/step - loss: 0.1232 - acc: 0.9500\n",
      "Epoch 95/100\n",
      "100/100 [==============================] - 0s 336us/step - loss: 0.0823 - acc: 0.9800\n",
      "Epoch 96/100\n",
      "100/100 [==============================] - 0s 326us/step - loss: 0.0698 - acc: 0.9800\n",
      "Epoch 97/100\n",
      "100/100 [==============================] - 0s 321us/step - loss: 0.1692 - acc: 0.9300\n",
      "Epoch 98/100\n",
      "100/100 [==============================] - 0s 446us/step - loss: 0.1439 - acc: 0.9500\n",
      "Epoch 99/100\n",
      "100/100 [==============================] - 0s 291us/step - loss: 0.1047 - acc: 0.9700\n",
      "Epoch 100/100\n",
      "100/100 [==============================] - 0s 396us/step - loss: 0.1307 - acc: 0.9500\n",
      "50/50 [==============================] - 0s 451us/step\n",
      "\n",
      "acc: 100.00%\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "one_hot_label_y_train = np_utils.to_categorical(y_train)\n",
    "one_hot_label_y_test = np_utils.to_categorical(y_test)\n",
    "model.fit(x_train, one_hot_label_y_train, epochs=epoch, batch_size=batch_size)\n",
    "score = model.evaluate(x_test, one_hot_label_y_test, batch_size=batch_size)\n",
    "print(\"\\n{}: {:.2f}%\".format(model.metrics_names[1], score[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict: versicolor,\n",
      "Actual: setosa,\n",
      "Is it Correct: False\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predict_data = np.array([4., 3., 4., 1.2])\n",
    "x = predict_data.reshape(-1,4)\n",
    "predict = model.predict(x)\n",
    "\n",
    "for i in range(len(predict)):    \n",
    "    guess = iris.target_names[np.argmax(predict[i])]\n",
    "    actual = iris.target_names[y_train[i]]\n",
    "    print(\"Predict: {},\\nActual: {},\\nIs it Correct: {}\\n\".format(guess, actual, guess==actual))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### to save the model \n",
    "we do this as if it saves time from retraining the model and runs on the last model that was ran "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save('my_model.h5') "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
